# test_rerankers.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æœ¬åœ°æ¨¡å‹çš„æ¨ç†ä¸QPSæ€§èƒ½
"""
å•å¡æµ‹è¯•è„šæœ¬ï¼šä¾æ¬¡éªŒè¯ bge-reranker-baseã€Qwen3-Reranker-4Bã€Qwen3-Reranker-8B 
åœ¨å„è‡ªå•å¼  GPU ä¸Šçš„æ¨ç†ä¸ QPS æ€§èƒ½ã€‚

ç›®å½•ç»“æ„ï¼š
  rerank_models/
  â”œâ”€ models -> /media/inspur/rerank_models/models
  â”‚    â”œâ”€ bge-reranker-base/
  â”‚    â”œâ”€ Qwen3-Reranker-4B/
  â”‚    â””â”€ Qwen3-Reranker-8B/
  â””â”€ test_rerankers.py  <- æœ¬è„šæœ¬

ä¾èµ–ï¼š
    pip install torch sentence-transformers modelscope

è¿è¡Œï¼š
    python test_rerankers.py
"""
import time
import torch
from sentence_transformers import CrossEncoder
from modelscope import AutoTokenizer, AutoModelForCausalLM

# ----------------------------
# GPU åˆ†é…: æ¯ä¸ªæ¨¡å‹å›ºå®šä½¿ç”¨ä¸€å¼  GPU
# ----------------------------
DEVICE_BGE     = torch.device("cuda:0")  # bge-reranker-base
DEVICE_QWEN4B  = torch.device("cuda:0")  # Qwen3-Reranker-4B
DEVICE_QWEN8B  = torch.device("cuda:1")  # Qwen3-Reranker-8B

# ----------------------------
# GPU ç›‘æ§å’ŒéªŒè¯å‡½æ•°
# ----------------------------
def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§å’ŒCUDAç¯å¢ƒ"""
    print("ğŸ” GPUç¯å¢ƒæ£€æŸ¥:")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"   æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
    print()

def get_gpu_memory(device):
    """è·å–æŒ‡å®šGPUçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if isinstance(device, torch.device):
        device_id = device.index
    else:
        device_id = device
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(device_id) / 1024**3
        reserved = torch.cuda.memory_reserved(device_id) / 1024**3
        return allocated, reserved
    return 0, 0

def print_gpu_usage(device, stage=""):
    """æ‰“å°GPUä½¿ç”¨æƒ…å†µ"""
    allocated, reserved = get_gpu_memory(device)
    device_id = device.index if isinstance(device, torch.device) else device
    print(f"   ğŸ“Š GPU {device_id} {stage}: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB")

def verify_model_on_gpu(model, device):
    """éªŒè¯æ¨¡å‹æ˜¯å¦çœŸæ­£åœ¨GPUä¸Š"""
    device_id = device.index if isinstance(device, torch.device) else device
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ‰€åœ¨è®¾å¤‡
    model_devices = set()
    for param in model.parameters():
        model_devices.add(param.device)
    
    print(f"   âœ… æ¨¡å‹å‚æ•°æ‰€åœ¨è®¾å¤‡: {model_devices}")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡å®šGPUä¸Š
    expected_device = torch.device(f"cuda:{device_id}")
    if expected_device in model_devices:
        print(f"   âœ… æ¨¡å‹å·²æ­£ç¡®åŠ è½½åˆ° GPU {device_id}")
        return True
    else:
        print(f"   âŒ æ¨¡å‹æœªåœ¨ GPU {device_id} ä¸Šï¼")
        return False

# ----------------------------
# æ¨¡å‹è·¯å¾„
# ----------------------------
BGE_MODEL   = "./models/bge-reranker-base"
QWEN3_4B    = "./models/Qwen3-Reranker-4B"
QWEN3_8B    = "./models/Qwen3-Reranker-8B"

# ----------------------------
# Qwen3-Reranker å‰ç¼€ä¸åç¼€å›ºå®šå­—ç¬¦ä¸²
# ----------------------------
PREFIX = (
    "<|im_start|>system\n"
    "Judge whether the Document meets the requirements based on the Query and the Instruct provided. "
    "Note that the answer can only be \"yes\" or \"no\"."
    "<|im_end|>\n"
    "<|im_start|>user\n"
)
SUFFIX = (
    "<|im_end|>\n"
    "<|im_start|>assistant\n"
    "<think>\n\n"
    "</think>\n\n"
)
MAX_LEN = 8192

# ----------------------------
# bge-reranker-base æµ‹è¯•
# ----------------------------
def test_bge(pairs):
    print(f"=== Testing bge-reranker-base on {DEVICE_BGE} ===")
    # è®°å½•åŠ è½½å‰çš„GPUçŠ¶æ€
    print_gpu_usage(DEVICE_BGE, "åŠ è½½å‰")
    
    # åŠ è½½æ¨¡å‹
    model = CrossEncoder(BGE_MODEL, device=DEVICE_BGE)
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
    verify_model_on_gpu(model.model, DEVICE_BGE)
    print_gpu_usage(DEVICE_BGE, "åŠ è½½å")
    
    # æ¨ç†æµ‹è¯•
    print("ğŸš€ å¼€å§‹æ¨ç†...")
    start = time.perf_counter()
    scores = model.predict(pairs)
    end = time.perf_counter()
    
    print_gpu_usage(DEVICE_BGE, "æ¨ç†å")
    
    for (q, d), s in zip(pairs, scores):
        print(f"[bge] â€œ{q}â€ â†” â€œ{d}â€ â‡’ {s:.4f}")
    print(f"bge-reranker-base QPS: {len(pairs)/(end-start):.1f}\n")

# ----------------------------
# Qwen3-Reranker æµ‹è¯•
# ----------------------------
def test_qwen(name, model_path, device, pairs):
    print(f"=== Testing {name} on {device} ===")
    
    # è®°å½•åŠ è½½å‰çš„GPUçŠ¶æ€
    print_gpu_usage(device, "åŠ è½½å‰")
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        padding_side='left',
        trust_remote_code=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True
    ).to(device).eval()
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
    verify_model_on_gpu(model, device)
    print_gpu_usage(device, "åŠ è½½å")
    
    # è·å– yes/no token id
    false_id = tokenizer.convert_tokens_to_ids('no')
    true_id  = tokenizer.convert_tokens_to_ids('yes')

    # æ„é€ å®Œæ•´æ–‡æœ¬ï¼ˆprefix + instruction + query + document + suffixï¼‰
    full_texts = []
    for q, d in pairs:
        text = (
            PREFIX +
            f"<Instruct>: Given a web search query, retrieve relevant passages that answer the query\n" +
            f"<Query>: {q}\n" +
            f"<Document>: {d}" +
            SUFFIX
        )
        full_texts.append(text)

    # æ¨ç†æµ‹è¯•
    print("ğŸš€ å¼€å§‹æ¨ç†...")
    start = time.perf_counter()
    inputs = tokenizer(
        full_texts,
        padding='longest',
        truncation=True,
        max_length=MAX_LEN,
        return_tensors='pt'
    )
    # ç§»åŠ¨åˆ°è®¾å¤‡
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # éªŒè¯è¾“å…¥æ•°æ®æ˜¯å¦åœ¨æ­£ç¡®çš„GPUä¸Š
    for key, tensor in inputs.items():
        print(f"   ğŸ“ è¾“å…¥ {key} åœ¨è®¾å¤‡: {tensor.device}")
    
    # æ¨ç†å¹¶æå–æœ€åä¸€ token logits
    with torch.no_grad():
        logits = model(**inputs).logits[:, -1, :]
        print(f"   ğŸ“ è¾“å‡º logits åœ¨è®¾å¤‡: {logits.device}")
        neg = logits[:, false_id]
        pos = logits[:, true_id]
        scores = torch.nn.functional.log_softmax(
            torch.stack([neg, pos], dim=1), dim=1
        )[:, 1].exp().tolist()
    end = time.perf_counter()

    print_gpu_usage(device, "æ¨ç†å")

    for (q, d), s in zip(pairs, scores):
        print(f"[{name}] â€œ{q}â€ â†” â€œ{d}â€ â‡’ {s:.4f}")
    print(f"{name} QPS: {len(pairs)/(end-start):.1f}\n")

# ----------------------------
# ä¸»æµç¨‹
# ----------------------------
if __name__ == '__main__':
    check_gpu_availability()  # æ£€æŸ¥GPUå¯ç”¨æ€§
    sample_pairs = [
        ("ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ", "ä»Šå¤©æ™´å¤©é€‚åˆå¤–å‡ºæ¸¸ç©"),
        ("ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ", "è‚¡å¸‚æ³¢åŠ¨å¾ˆå¤§"),
        ("Pythonæ˜¯ä»€ä¹ˆè¯­è¨€ï¼Ÿ", "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€"),
        ("Pythonæ˜¯ä»€ä¹ˆè¯­è¨€ï¼Ÿ", "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½"),
    ]
    test_bge(sample_pairs)
    test_qwen('Qwen3-Reranker-4B', QWEN3_4B, DEVICE_QWEN4B, sample_pairs)
    test_qwen('Qwen3-Reranker-8B', QWEN3_8B, DEVICE_QWEN8B, sample_pairs)
    print("âœ… All models tested.")